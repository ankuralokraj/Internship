{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b357f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException, StaleElementReferenceException, ElementNotInteractableException\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80576947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1. Answer:- Scrape the details of most viewed videos on YouTube from Wikipedia and  need to find following details: A) Rank B) Name C) Artist D) Upload date E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url=(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3167a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for scraping data\n",
    "Rank =[]\n",
    "Name =[]\n",
    "Artist =[]\n",
    "Upload_date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank Via X path\n",
    "try:\n",
    "    rank=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Rank.append('NA')\n",
    "    \n",
    "# Extracting Name Via X path\n",
    "try:\n",
    "    name=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Name.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Name.append('NA')\n",
    "    \n",
    "# Extracting Artist Name Via Xpath\n",
    "try:\n",
    "    artist=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[3]\")\n",
    "    for i in artist:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Artist.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Artist.append('NA')\n",
    "    \n",
    "# Extracting Upload date Via Xpath\n",
    "try:\n",
    "    upload_date=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[5]\")\n",
    "    for i in upload_date:\n",
    "        Upload_date.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Upload_date.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Upload_date.append('NA') \n",
    "\n",
    "# Extracting Views via Xpath\n",
    "try:\n",
    "    views=driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\")\n",
    "    for i in views:\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementExceptionhElementException:\n",
    "    Views.append('NA')\n",
    "except StaleElementReferenceException:\n",
    "    Views.append('NA')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c744e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Rank),len(Name),len(Artist),len(Views),len(Upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de872a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for scrap data\n",
    "Wiki_YT=pd.DataFrame({'Rank':Rank,'Video Name':Name,'Uploader':Artist,'Views (in Billons)':Views,'Upload Date':Upload_date})\n",
    "print('\\033[1m'+'Most Viewed Video on YouTube from Wikipedia :'+'\\033[0m')\n",
    "Wiki_YT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2511102",
   "metadata": {},
   "source": [
    "# 2. Scrape the details teamIndiaâ€™sinternationalfixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def India_fixtures():\n",
    "    \"\"\"Creating function that searches details of Team India International Fixtures\"\"\"\n",
    "\n",
    "    template = 'https://www.bcci.tv/.' #URL template for accessing the website\n",
    "    \n",
    "    driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    driver.get(template) #Opening with the URL template\n",
    "    driver.maximize_window() #Maximize the Window\n",
    "    \n",
    "    time.sleep(5) #Making the Function wait for 5sec so webpage will open\n",
    "    \n",
    "    #Creating a variable \"Button\" to search the Fixtures and getting the 'href' to open in the driver.\n",
    "    button = driver.find_elements(By.XPATH,\"//div[@class = 'navigation__drop-down drop-down drop-down--reveal-on-hover']/div/ul/li/a\")\n",
    "    \n",
    "    button.get_attribute[\"href\"]\n",
    "    \n",
    "    time.sleep(5) #Making the Function wait for 5sec so webpage will open\n",
    "    \n",
    "    Match_title = [] #Creating Variable Match title to collect the title of the match i.e. Test or ODI.\n",
    "    Series = [] #Creating Variable Series to collect the Series name. \n",
    "    Place = [] #Creating variable Place to collect the Match venue.\n",
    "    Date = [] #Creating Variable Date to collect the venue Date.\n",
    "    Time = [] #Creating Variable Time to collect the venue Time.\n",
    "    details = [] #Creating a Dummy variable Details to collect the Date and time details\n",
    "    \n",
    "    #Scraping Match Details and appending in the Match list.\n",
    "    for i in driver.find_element(By.XPATH,\"//div [@class = 'fixture__format-strip']/ span [@class = 'u-unskewed-text fixture__format']\"):\n",
    "        Match_title.append(i.text)\n",
    "\n",
    "    #Scraping Series Details and appending in the Series list.    \n",
    "    for i in driver.find_elements(By.XPATH,\"//div [@class = 'fixture__format-strip']/ span [@class = 'u-unskewed-text fixture__tournament-label u-truncated']\"):\n",
    "        Series.append(i.text)\n",
    "        \n",
    "   #Scraping Place Details and appending in the Place list.\n",
    "    for i in driver.find_element(By.XPATH,\"//div [@class = 'fixture__description u-unskewed-text']/p/span\"):\n",
    "        Place.append(i.text)\n",
    "    \n",
    "    #Scraping Date and Time Details and appending in the details list.\n",
    "    for i in driver.find_element(By.XPATH,\"//div [@class = 'fixture__datetime desktop-only']\"):\n",
    "        details.append(i.text.replace('\\n',' '))\n",
    "        \n",
    "    driver.close() #Exiting the driver post scraping the information    \n",
    "    \n",
    "    #Spliting the details list and sorting it in Date and Time. \n",
    "    Date = [i.split(' ',3)[:3]for i in details]\n",
    "    Date = [' '.join(i) for i in Date]\n",
    "    Time = [i.split(' ',3)[-1]for i in details]\n",
    "    \n",
    "    #Creating the DataFrame from all collected data\n",
    "    Fixtures = pd.DataFrame({'Match title' : Match_title , \n",
    "                             'Series' : Series , \n",
    "                             'Place' : Place, \n",
    "                             'Date' : Date ,\n",
    "                             'Time' : Time})\n",
    "    \n",
    "    return Fixtures #returns the DataFrame with collected details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "India_fixtures()#Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006594d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d93ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70abe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e9ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a94ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723477d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40bf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b7d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1434fee7",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP at current price (19-20)\n",
    "\n",
    "D) GSDP at current price (18-19)\n",
    "\n",
    "E) Share(18-19)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfe2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def State_wise_GDP():\n",
    "    \"\"\"Creating function that searches details of India State_wise_GDP\"\"\"\n",
    "    \n",
    "    template = 'http://statisticstimes.com/'#URL template for accessing the website.\n",
    "    driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    driver.get(template) #Opening with the URL template.\n",
    "    driver.maximize_window() #Maximize the Window.\n",
    "    \n",
    "    href = [] #Declaring the list href to scrap all the href details\n",
    "    for i in driver.find_elements(By.XPATH,\"//div [@class = 'dropdown']/div [@class = 'dropdown-content']/a\"):\n",
    "        href.append(i.get_attribute('href'))\n",
    "        \n",
    "    driver.get(href[4]) #opening the href[4] which is GDP of India.\n",
    "    \n",
    "    href = [] #Declaring the list href to scrap all the href details of GDP of Indian states.\n",
    "    for i in driver.find_elements(By.XPATH,\"//div [@style = 'float:left;background-color:seashell;width:400px;height:800px;']/ul [@style = 'list-style-type:none;margin-left:20px;']/li/a\"):\n",
    "        if i.text == 'Â» GDP of Indian states':\n",
    "            href.append(i.get_attribute('href'))\n",
    "    \n",
    "    driver.get(href[0]) #opening the href[4] which is GDP of Indian states.\n",
    "    \n",
    "    Rank = [] #Declaring Rank list to collect the Rank details\n",
    "    State = [] #Declaring State list to collect the State details\n",
    "    GSDP20 = [] #Declaring GSDP20 list to collect the GSDP20 details\n",
    "    GSDP19 = [] #Declaring GSDP19 list to collect the GSDP19 details\n",
    "    Share = [] #Declaring Share list to collect the Share details\n",
    "    GDP = [] #Declaring GDP list to collect the GDP details\n",
    "    details = [] #scraping all the above mentioned details and storing it in the details list.\n",
    "    for i in driver.find_elements(By.XPATH,\"//tbody /tr [@role = 'row']/td\"):\n",
    "        details.append(i.text)\n",
    "    \n",
    "    driver.close() #Exiting the Driver post collecting all the details.\n",
    "    \n",
    "    details = details[:264] #Sorting the list with the required details.\n",
    "    \n",
    "    z= 0\n",
    "    while z < len(details): # Splitting the collected details and storing it in the required list.\n",
    "        Rank.append(details[z])\n",
    "        z+=1\n",
    "        State.append(details[z])\n",
    "        z+=1\n",
    "        GSDP20.append(details[z])\n",
    "        z+=1\n",
    "        GSDP19.append(details[z])\n",
    "        z+=1\n",
    "        Share.append(details[z])\n",
    "        z+=1\n",
    "        GDP.append(details[z])\n",
    "        z+=3\n",
    "    \n",
    "    #Creating a DataFrame with collected details.\n",
    "    table = pd.DataFrame({\"Rank\" : Rank,\n",
    "                          \"State\" : State,\n",
    "                          \"GSDP(19-20)\" : GSDP20,\n",
    "                          \"GSDP(18-19)\" : GSDP19,\n",
    "                          \"Share(2018)\" : Share,\n",
    "                          \"GDP($ billion)\": GDP})\n",
    "    \n",
    "    return table #returing the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6448cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = State_wise_GDP() #Calling the function and storing it in a variable.\n",
    "GDP.head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41d18c",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to webdriver\n",
    "driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening Wikipedia webpage\n",
    "url='https://github.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on trending option\n",
    "trending=driver.find_elements(By.XPATH,'//ul[@class=\"list-style-none mb-3\"][2]/li[3]/a')\n",
    "try:\n",
    "    trending.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db559f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82061ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c596d47",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. \n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_100():\n",
    "    \"\"\"Creating function that searches details of top 100 songs from billboard\"\"\"\n",
    "    \n",
    "    template = 'https://www.billboard.com/'#URL template for accessing the website.\n",
    "    driver = webdriver.Chrome()#Calling the Web Driver.\n",
    "    driver.get(template)#Opening with the URL template.\n",
    "    driver.maximize_window()#Maximize the Window.\n",
    "    \n",
    "    href = []#Creating href list to collect \"HOT 100\" href details.\n",
    "    for i in driver.find_elements(By.XPATH,\"//li [@class = 'header__subnav__item']/a\"):\n",
    "        if i.text == 'HOT 100':\n",
    "            href.append(i.get_attribute('href'))\n",
    "        \n",
    "    driver.get(href[0])# Opening the Driver with the collected href detail.\n",
    "    \n",
    "    #Scraping the Song_name and storing it in the Song_name list.\n",
    "    Song_name = [i.text for i in driver.find_elements(By.XPATH,\"// span [@class ='chart-element__information']/span[@ class= 'chart-element__information__song text--truncate color--primary']\")]\n",
    "    \n",
    "    #Scraping the Artist_name and storing it in the Artist_name list.\n",
    "    Artist_name = [i.text for i in driver.find_elements(By.XPATH,\"// span [@class ='chart-element__information']/span[@ class= 'chart-element__information__artist text--truncate color--secondary']\")]\n",
    "    \n",
    "    #Scraping the Last_week_rank and storing it in the Last_week_rank list.\n",
    "    Last_week_rank = [i.text for i in driver.find_elements(\"//div[@class = 'chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[@class = 'chart-element__meta text--center color--secondary text--last']\")]\n",
    "    \n",
    "    #Scraping the Peak_rank and storing it in the Peak_rank list.\n",
    "    Peak_rank = [i.text for i in driver.find_elements(By.XPATH,\"//div[@class = 'chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[@class = 'chart-element__meta text--center color--secondary text--peak']\")]\n",
    "    \n",
    "    #Scraping the Weeks_on_board and storing it in the Weeks_on_board list.\n",
    "    Weeks_on_board = [i.text for i in driver.find_elements(By.XPATH,\"//div[@class = 'chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[@class = 'chart-element__meta text--center color--secondary text--week']\")]\n",
    "    \n",
    "    driver.close()#Quiting the Driver post Scraping the Details Required.\n",
    " #Creating a DataFrame withe the collected Details.\n",
    "    table = pd.DataFrame({\"Song_name\" : Song_name,\n",
    "                          \"Artist_name\" : Artist_name,\n",
    "                          \"Last_week_rank\" : Last_week_rank,\n",
    "                          \"Peak_rank\" : Peak_rank,\n",
    "                          \"Weeks_on_board\": Weeks_on_board})\n",
    "    \n",
    "    return table #Retuting the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd351d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100songs = top_100() #Calling the Function and assigning it to a variable.\n",
    "top_100songs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35e92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56744798",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Highest_selling_novels():\n",
    "    \"\"\"The Function which searches and returns top selling novels from theguardian.com\"\"\"\n",
    "    \n",
    "    #URL template for accessing the website.\n",
    "    template = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/'\n",
    "    driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    driver.get(template)#Opening with the URL template.\n",
    "    driver.maximize_window()#Maximize the Window.\n",
    "    \n",
    "    #Creating a dummy list and scraping the all the required details.\n",
    "    dummy = [i.text for i in driver.find_elements(By.XPATH,\"//table [@class ='in-article sortable']//tr/td\")]\n",
    "    \n",
    "    driver.close() #Quiting the driver post collecting the details. \n",
    "    \n",
    "    dummy.remove('SOURCE: NIELSEN BOOK SCAN') #Removing the unwanted details from dummy list. \n",
    "\n",
    "    Book_name = [] #Creating list Book_name to collect the Book_name details.\n",
    "    Author_name = [] #Creating list Author_name to collect the Author_name details.\n",
    "    Volumes_sold = [] #Creating list Volumes_sold to collect the Volumes_sold details.\n",
    "    Publisher = [] #Creating list Publisher to collect the Publisher details.\n",
    "    Genre = [] #Creating list Genre to collect the Genre details.\n",
    "\n",
    "    z = 0\n",
    "    while z < len(dummy): #Splitting dummy list and storing the required details it in the appropriate list.\n",
    "        z+=1\n",
    "        Book_name.append(dummy[z])\n",
    "        z+=1\n",
    "        Author_name.append(dummy[z])\n",
    "        z+=1\n",
    "        Volumes_sold.append(dummy[z])\n",
    "        z+=1\n",
    "        Publisher.append(dummy[z])\n",
    "        z+=1\n",
    "        Genre.append(dummy[z])\n",
    "        z+=1\n",
    "     #Creating the DataFrame with all the collected details.\n",
    "    table = pd.DataFrame({\"Book_name\" : Book_name,\n",
    "                          \"Author_name\" : Author_name,\n",
    "                          \"Volumes_sold\" : Volumes_sold,\n",
    "                          \"Publisher\" : Publisher,\n",
    "                          \"Genre\": Genre})\n",
    "    \n",
    "    return table #returning the DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3da58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Highest_selling_novels()#calling the Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8231d",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6494f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostwatched_tv_series():\n",
    "    \"\"\"The Function which searches and returns top selling novels from theguardian.com\"\"\"\n",
    "\n",
    "    template = 'https://www.imdb.com/list/ls095964455/' #URL template for accessing the website.\n",
    "    driver = webdriver.Chrome() #Calling the Web Driver.\n",
    "    driver.get(template) #Opening with the URL template.\n",
    "    driver.maximize_window() #Maximize the Window.\n",
    "    \n",
    "    #Creating the list Name and scraping the name of TV shows.\n",
    "    Name = [i.text for i in driver.find_elements(By.XPATH,\"//h3 [@class ='lister-item-header']/a\")]\n",
    "    \n",
    "    #Creating the list Year_span and scraping the Year_span of TV shows.\n",
    "    Year_span = [i.text for i in driver.find_elements(By.XPATH,\"//h3 [@class ='lister-item-header']/span[@class = 'lister-item-year text-muted unbold']\")]\n",
    "    \n",
    "    #Creating the list Genre and scraping the Genre of TV shows.\n",
    "    Genre = [i.text for i in driver.find_elements(By.XPATH,\"//p [@class ='text-muted text-small']/span[@class = 'genre']\")]\n",
    "    \n",
    "    #Creating the list Run_time and scraping the Run_time of TV shows.\n",
    "    Run_time = [i.text for i in driver.find_elements(By.XPATH,\"//p [@class ='text-muted text-small']/span[@class = 'runtime']\")]\n",
    "    \n",
    "    #Creating the list Ratings and scraping the Ratings of TV shows.\n",
    "    Ratings = [i.text for i in driver.find_elements(By.XPATH,\"//div [@class ='ipl-rating-star small']/span[@class = 'ipl-rating-star__rating']\")]\n",
    "    \n",
    "    #Creating the list Votes and scraping the Votes of TV shows.\n",
    "    Votes = [i.text for i in driver.find_elements(By.XPATH,\"//p [@class ='text-muted text-small']/span[@name = 'nv']\")]\n",
    "    \n",
    "    driver.quit()#exiting the driver post scraping the information\n",
    "    \n",
    "    #Creating a Dataframe with all collected details.\n",
    "    table = pd.DataFrame({\"Name\" : Name,\n",
    "                          \"Year_span\" : Year_span,\n",
    "                          \"Genre\" : Genre,\n",
    "                          \"Run_time\" : Run_time,\n",
    "                          \"Ratings\": Ratings,\n",
    "                          \"Votes\" : Votes})\n",
    "    \n",
    "    return table #Returning the DataFrame.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostwatched_tv_series() #calling the Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273900bc",
   "metadata": {},
   "source": [
    "# 8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc84b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Repository():\n",
    "    \"\"\"The Function that Search all the ML Repository details and return them in DataFrame\"\"\"\n",
    "\n",
    "    template = 'https://archive.ics.uci.edu/' #URL template for accessing the website.\n",
    "    driver = webdriver.Chrome() #Calling the Web Driver.\n",
    "    driver.get(template) #Opening with the URL template.\n",
    "    driver.maximize_window() #Maximize the Window.\n",
    "    time.sleep(3)#making the function to wait for 3sec\n",
    "    \n",
    "    #finding view all repository and clicking the same.\n",
    "    ml_repository = driver.find_element(By.XPATH,\"//span [@class ='normal']/b/a\")\n",
    "    ml_repository.click()\n",
    "    \n",
    "    #creating a dummy list 'details' and scraping Dataset_name, Data_type, Task, Attribute_type, Noof_instances, Noof_attribute, Year.\n",
    "    details = [i.text for i in driver.find_elements(By.XPATH,\"//table [@border ='1']/tbody/tr/td\")]\n",
    "    del details[:7] #Removing the unwanted details.\n",
    "    \n",
    "    driver.close() #Closing the Driver post scraping the details.\n",
    "    \n",
    "    Dataset_name = [] #creating list Dataset_name to collect Dataset_name. \n",
    "    Data_type = [] #creating list Data_type to collect Data_type. \n",
    "    Task = [] #creating list Task to collect Task. \n",
    "    Attribute_type = [] #creating list Attribute_type to collect Attribute_type. \n",
    "    Noof_instances = [] #creating list Noof_instances to collect Noof_instances. \n",
    "    Noof_attribute = [] #creating list Noof_attribute to collect Noof_attribute. \n",
    "    Year = [] #creating list Year to collect Year. \n",
    "\n",
    "    z= 0\n",
    "    while z < len(details): #Splitting the Dummy variable and storing it in a appropriate list.\n",
    "        Dataset_name.append(details[z])\n",
    "        z +=1\n",
    "\n",
    "        Data_type.append(details[z])\n",
    "        z +=1\n",
    "    \n",
    "        Task.append(details[z])\n",
    "        z +=1\n",
    "\n",
    "        Attribute_type.append(details[z])\n",
    "        z +=1\n",
    "\n",
    "        Noof_instances.append(details[z])\n",
    "        z +=1\n",
    "    \n",
    "        Noof_attribute.append(details[z])\n",
    "        z +=1\n",
    "\n",
    "        Year.append(details[z])\n",
    "        z +=1\n",
    "        \n",
    "        \n",
    "    #Creating DataFrame with collected Details    \n",
    "    table = pd.DataFrame({\"Dataset_name\" : Dataset_name,\n",
    "                          \"Data_type\" : Data_type,\n",
    "                          \"Task\" : Task,\n",
    "                          \"Attribute_type\" : Attribute_type,\n",
    "                          \"Noof_instances\" : Noof_instances,\n",
    "                          \"Noof_attribute\" : Noof_attribute,\n",
    "                          \"Year\" : Year})\n",
    "        \n",
    "    table = table.replace(' ', '-')\n",
    "        \n",
    "    return table #returning the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a57893",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCI_Repository = UCI_Repository() #Calling the function and storing it in a variable.\n",
    "UCI_Repository.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9973fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"C:\\Users\\Baby Boss\\Desktop\\internshit flip robo\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/hr-recruiters-consultants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92793a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/div/div[1]/div[1]/div[2]/input')\n",
    "search.send_keys('Data Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a617e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "find=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/button')\n",
    "find.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link=[]\n",
    "for i in driver.find_elements(By.XPATH,'//a[@class=\"ellipsis\"]'):\n",
    "    link.append(i.get_attribute('href'))\n",
    "len(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for i in link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        recruiter=driver.find_element(By.XPATH,'//h1[@class=\"fl ellipsis wLimit hd\"]')\n",
    "        names.append(recruiter.text.replace('-',''))\n",
    "    except NoSuchElementException:\n",
    "        names.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d41b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=[]\n",
    "for i in link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        D=driver.find_element(By.XPATH,'//div[@class=\"ellipsis\"]')\n",
    "        designation.append(D.text.replace('-',''))\n",
    "    except NoSuchElementException:\n",
    "        designation.append('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "company=[]\n",
    "for i in link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        C=driver.find_element(By.XPATH,'//a[@class=\"fl ellipsis widLrg\"]')\n",
    "        company.append(C.text.replace('-',''))\n",
    "    except NoSuchElementException:\n",
    "        company.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills=[]\n",
    "for i in link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        S=driver.find_element(By.XPATH,'//div[@class=\"fl lPortn\"]//p')\n",
    "        skills.append(S.text.replace('-',''))\n",
    "    except NoSuchElementException:\n",
    "        skills.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=[]\n",
    "for i in link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        L=driver.find_element(By.XPATH,'//h1[@class=\"f1 ellipsis loc\"]')\n",
    "        location.append(L.text.replace('-',''))\n",
    "    except NoSuchElementException:\n",
    "        location.append('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(names),len(designation),len(company),len(skills),len(location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'NAME':names,'DESIGNATION':designation,'COMPANY':company,'SKILLS':skills,'LOCATION':location})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
